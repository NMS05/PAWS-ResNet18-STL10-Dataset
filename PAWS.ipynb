{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XnhjXCRx6Hzo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja1MNAFI1OXo"
      },
      "source": [
        "# PAWS TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPluJU9sVG0_"
      },
      "source": [
        "### STL10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "83ThMXIUYAre"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "unlabeled_batch_size = 128\n",
        "support_samples_per_class = 5\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFo7W26W7lVi",
        "outputId": "3feaf71d-538d-4fbe-d2f8-67a16b0bba4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "unlabeled_data = torchvision.datasets.STL10('./content',split='unlabeled',download=True,transform=T.ToTensor())\n",
        "labeled_data = torchvision.datasets.STL10('./content',split='train',download=True,transform=T.ToTensor())\n",
        "test_data = torchvision.datasets.STL10('./content',split='test',download=True,transform=T.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6b2JEcaQ9xB"
      },
      "source": [
        "Class balanced sampling of support samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YiXqs1uCMgig"
      },
      "outputs": [],
      "source": [
        "# Reference - https://discuss.pytorch.org/t/load-the-same-number-of-data-per-class/65198\n",
        "class BalancedBatchSampler(BatchSampler):\n",
        "    def __init__(self, dataset, n_classes, n_samples):\n",
        "        loader = DataLoader(dataset) # BS = 1 by default\n",
        "        self.labels_list = []\n",
        "        for _, label in loader:\n",
        "            self.labels_list.append(label) # extract all labels in the dataset\n",
        "        self.labels = torch.LongTensor(self.labels_list) # dtype of LongTensor = int64\n",
        "        self.labels_set = list(set(self.labels.numpy())) # converts tensor(self.label) to numpy.... set() extracts all unique elements in a np array and then it is converted to a list.\n",
        "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
        "                                 for label in self.labels_set} # a dictionary of indices (images) for each class label (0,1,2..)\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l]) # shuffle all indices\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set} # makes sure they donot overlap upon iter.\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < len(self.dataset):\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False) # choose unique labels randomly\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][ # extend is similar to append operation. for every class_label indices of length n_samples is extracted.\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]): # reset upon depletion\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices # simlar to return but it is of the type object:generator\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZIcLlE9-eiF8"
      },
      "outputs": [],
      "source": [
        "balanced_batch_sampler = BalancedBatchSampler(labeled_data, num_classes, support_samples_per_class)\n",
        "support_dataloader = torch.utils.data.DataLoader(labeled_data, batch_sampler=balanced_batch_sampler)\n",
        "unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_data, batch_size=unlabeled_batch_size, shuffle=True)\n",
        "\n",
        "# use the following format for loading support samples alongside unlabeled samples\n",
        "# support_loader = iter(support_dataloader)\n",
        "# x,y = next(support_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SviRE_KVCdu"
      },
      "source": [
        "DATA AUGMENTATION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "p1HtQbJ27qVR"
      },
      "outputs": [],
      "source": [
        "class DataAugmentation:\n",
        "  def __init__(self, img_size=96, crop_scale=(0.3,1.0), s=1): # s - jitter strenth for unsupervised\n",
        "\n",
        "    jitter1 = T.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "\n",
        "    self.transforms1 = torch.nn.Sequential(\n",
        "        T.RandomResizedCrop(size=img_size, scale=crop_scale), \n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomApply([jitter1], p=0.8),\n",
        "        T.Normalize(mean=[0.49139968, 0.48215827 ,0.44653124], std=[0.24703233, 0.24348505, 0.26158768])\n",
        "    )\n",
        "    self.transforms2 = torch.nn.Sequential( \n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.Normalize(mean=[0.49139968, 0.48215827 ,0.44653124], std=[0.24703233, 0.24348505, 0.26158768])\n",
        "    )  \n",
        "  def __call__(self, x, sx):\n",
        "    return (self.transforms1(x),self.transforms1(x), self.transforms2(sx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLVNbJdilnrp"
      },
      "source": [
        "### MODEL and PAWS loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CnwuMzy8lnNp"
      },
      "outputs": [],
      "source": [
        "class PAWS_ResNet18(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(PAWS_ResNet18, self).__init__()\n",
        "\n",
        "        resnet18 = torchvision.models.resnet18(pretrained=False)\n",
        "        resnet18.fc = nn.Linear(resnet18.fc.in_features,128,bias=True)\n",
        "        self.base_model = resnet18\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(in_features=128, out_features=128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=256),\n",
        "        )\n",
        "\n",
        "        self.augmentation = DataAugmentation()\n",
        "\n",
        "    def forward(self,x, support_x):\n",
        "      x1,x2, sx = self.augmentation(x, support_x)\n",
        "      feature1 = self.projection(self.base_model(x1))\n",
        "      feature2 = self.projection(self.base_model(x2))\n",
        "      support = self.projection(self.base_model(sx))\n",
        "\n",
        "      return (feature1,feature2,support)\n",
        "\n",
        "model = PAWS_ResNet18()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_bSQ1LBPWVbh"
      },
      "outputs": [],
      "source": [
        "class PAWS_loss(nn.Module):\n",
        "    def __init__(self, tau=0.1, T=0.25):\n",
        "        super(PAWS_loss, self).__init__()\n",
        "        self.tau = tau\n",
        "        self.T = T\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def sharpen(self, p):\n",
        "        sharp_p = p**(1./self.T)\n",
        "        sharp_p /= torch.sum(sharp_p, dim=1, keepdim=True)\n",
        "        return sharp_p\n",
        "    \n",
        "    def snn(self, query, supports, labels):\n",
        "        # Step 1: normalize embeddings\n",
        "        query = torch.nn.functional.normalize(query)\n",
        "        supports = torch.nn.functional.normalize(supports)\n",
        "        # Step 2: compute similarlity between local embeddings\n",
        "        return self.softmax(query @ supports.T / self.tau) @ labels\n",
        "    \n",
        "    def forward(self, anchor_views,\n",
        "        anchor_supports,\n",
        "        anchor_support_labels,\n",
        "        target_views,\n",
        "        target_supports,\n",
        "        target_support_labels):\n",
        "      \n",
        "        # Step 1: compute anchor predictions\n",
        "        probs = self.snn(anchor_views, anchor_supports, anchor_support_labels)\n",
        "\n",
        "        # Step 2: compute targets for anchor predictions\n",
        "        with torch.no_grad():\n",
        "            targets = self.snn(target_views, target_supports, target_support_labels) # Note: target views and target supports must be detached.\n",
        "            targets = self.sharpen(targets)\n",
        "\n",
        "        # Step 3: compute cross-entropy loss H(targets, queries)\n",
        "        loss = torch.mean(torch.sum(torch.log(probs**(-targets)), dim=1))\n",
        "\n",
        "        # Step 4: compute me-max regularizer\n",
        "        rloss = 0.\n",
        "        avg_probs = torch.mean(self.sharpen(probs), dim=0)\n",
        "        rloss -= torch.sum(torch.log(avg_probs**(-avg_probs)))\n",
        "\n",
        "        return loss, rloss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPiDVHrT6FKr"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qS9vk-YrLuCW"
      },
      "outputs": [],
      "source": [
        "loss_fn = PAWS_loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7rSdlGx6ES8",
        "outputId": "55fedc1e-4305-4087-e0a2-b60161d31e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch.......................................... 1\n",
            "\tCross Entropy Loss =  10.1\n",
            "\tMean Entropy =  13.81\n",
            "\n",
            "Epoch.......................................... 2\n",
            "\tCross Entropy Loss =  9.34\n",
            "\tMean Entropy =  13.86\n",
            "\n",
            "Epoch.......................................... 3\n",
            "\tCross Entropy Loss =  9.09\n",
            "\tMean Entropy =  13.86\n",
            "\n",
            "Epoch.......................................... 4\n",
            "\tCross Entropy Loss =  8.89\n",
            "\tMean Entropy =  13.87\n",
            "\n",
            "Epoch.......................................... 5\n",
            "\tCross Entropy Loss =  8.74\n",
            "\tMean Entropy =  13.87\n",
            "\n",
            "Epoch.......................................... 6\n",
            "\tCross Entropy Loss =  8.44\n",
            "\tMean Entropy =  13.88\n",
            "\n",
            "Epoch.......................................... 7\n",
            "\tCross Entropy Loss =  8.56\n",
            "\tMean Entropy =  13.88\n",
            "\n",
            "Epoch.......................................... 8\n",
            "\tCross Entropy Loss =  8.44\n",
            "\tMean Entropy =  13.88\n",
            "\n",
            "Epoch.......................................... 9\n",
            "\tCross Entropy Loss =  8.28\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 10\n",
            "\tCross Entropy Loss =  8.08\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 11\n",
            "\tCross Entropy Loss =  8.2\n",
            "\tMean Entropy =  13.88\n",
            "\n",
            "Epoch.......................................... 12\n",
            "\tCross Entropy Loss =  8.14\n",
            "\tMean Entropy =  13.88\n",
            "\n",
            "Epoch.......................................... 13\n",
            "\tCross Entropy Loss =  8.04\n",
            "\tMean Entropy =  13.88\n",
            "\n",
            "Epoch.......................................... 14\n",
            "\tCross Entropy Loss =  7.9\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 15\n",
            "\tCross Entropy Loss =  8.04\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 16\n",
            "\tCross Entropy Loss =  7.89\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 17\n",
            "\tCross Entropy Loss =  7.8\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 18\n",
            "\tCross Entropy Loss =  7.86\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 19\n",
            "\tCross Entropy Loss =  7.76\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 20\n",
            "\tCross Entropy Loss =  7.74\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 21\n",
            "\tCross Entropy Loss =  7.67\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 22\n",
            "\tCross Entropy Loss =  7.51\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 23\n",
            "\tCross Entropy Loss =  7.48\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 24\n",
            "\tCross Entropy Loss =  7.54\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 25\n",
            "\tCross Entropy Loss =  7.43\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 26\n",
            "\tCross Entropy Loss =  7.47\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 27\n",
            "\tCross Entropy Loss =  7.29\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 28\n",
            "\tCross Entropy Loss =  7.36\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 29\n",
            "\tCross Entropy Loss =  7.24\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 30\n",
            "\tCross Entropy Loss =  7.19\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 31\n",
            "\tCross Entropy Loss =  7.19\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 32\n",
            "\tCross Entropy Loss =  7.2\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 33\n",
            "\tCross Entropy Loss =  7.12\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 34\n",
            "\tCross Entropy Loss =  7.09\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 35\n",
            "\tCross Entropy Loss =  7.3\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 36\n",
            "\tCross Entropy Loss =  7.1\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 37\n",
            "\tCross Entropy Loss =  7.25\n",
            "\tMean Entropy =  13.89\n",
            "\n",
            "Epoch.......................................... 38\n",
            "\tCross Entropy Loss =  7.05\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 39\n",
            "\tCross Entropy Loss =  7.19\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 40\n",
            "\tCross Entropy Loss =  7.1\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 41\n",
            "\tCross Entropy Loss =  6.97\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 42\n",
            "\tCross Entropy Loss =  6.93\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 43\n",
            "\tCross Entropy Loss =  6.98\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 44\n",
            "\tCross Entropy Loss =  6.87\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 45\n",
            "\tCross Entropy Loss =  7.01\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 46\n",
            "\tCross Entropy Loss =  6.82\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 47\n",
            "\tCross Entropy Loss =  6.84\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 48\n",
            "\tCross Entropy Loss =  6.99\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 49\n",
            "\tCross Entropy Loss =  6.92\n",
            "\tMean Entropy =  13.9\n",
            "\n",
            "Epoch.......................................... 50\n",
            "\tCross Entropy Loss =  6.83\n",
            "\tMean Entropy =  13.9\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # Reset Loss Values \n",
        "  cross_ent_loss = 0.0\n",
        "  memax_loss = 0.0\n",
        "\n",
        "  model.train()\n",
        "  \n",
        "  support_loader = iter(support_dataloader) # for every epoch, prepare the support set\n",
        "\n",
        "  # Load unlabeled data\n",
        "  for X,_ in unlabeled_dataloader:\n",
        "\n",
        "    # load support samples\n",
        "    try:\n",
        "      sx,sy = next(support_loader) # load support samples\n",
        "    except StopIteration:\n",
        "      support_loader = iter(support_dataloader) # reset when support samples are deleted\n",
        "      sx,sy = next(support_loader)\n",
        "\n",
        "    # load tensors to device\n",
        "    X = X.to(device)\n",
        "    sy = nn.functional.one_hot(sy,num_classes=num_classes).float() # convert longint to float\n",
        "    sx = sx.to(device)\n",
        "    sy = sy.to(device)\n",
        "\n",
        "    #Reseting Gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Predictions\n",
        "    anchor, target, supports = model(X,sx)\n",
        "\n",
        "    # Calculate Loss\n",
        "    ent, memax = loss_fn(anchor, supports, sy, target.detach(), supports.detach(), sy)\n",
        "\n",
        "    cross_ent_loss += ent.item()\n",
        "    memax_loss += memax.item()\n",
        "    _loss = ent + memax\n",
        "\n",
        "    # Update Parameters\n",
        "    _loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(\"\\nEpoch..........................................\", epoch + 1)\n",
        "  print(\"\\tCross Entropy Loss = \", round(cross_ent_loss/float(unlabeled_batch_size),2))\n",
        "  print(\"\\tMean Entropy = \", round(memax_loss/-float(unlabeled_batch_size),2))\n",
        "\n",
        "torch.save(model.base_model.state_dict(),'paws_weights.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_SviRE_KVCdu"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
